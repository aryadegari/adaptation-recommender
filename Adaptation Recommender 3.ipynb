{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations for python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time to create venv and install packages\n",
    "\n",
    "# %conda create -n adrec-dev anaconda -y\n",
    "# %conda activate adrec-dev -y\n",
    "\n",
    "# %conda install -n adrec-dev pandas -y\n",
    "\n",
    "# %conda install pip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models and train/test logs\n",
    "we assume that the trained models are trained for each train log and ready to be tested by test logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model: synthetic_logs_with_adaptations_20_percent.joblib\n",
      "load_logs: synthetic_logs_with_adaptations_20_percent_test.csv\n",
      "load_logs: synthetic_logs_with_adaptations_20_percent_train.csv\n",
      "load_model: synthetic_logs_with_adaptations_40_percent.joblib\n",
      "load_logs: synthetic_logs_with_adaptations_40_percent_test.csv\n",
      "load_logs: synthetic_logs_with_adaptations_40_percent_train.csv\n",
      "load_model: synthetic_logs_with_adaptations_66_percent.joblib\n",
      "load_logs: synthetic_logs_with_adaptations_66_percent_test.csv\n",
      "load_logs: synthetic_logs_with_adaptations_66_percent_train.csv\n",
      "load_model: bpic17_logs_with_interventions_20_percent.joblib\n",
      "load_logs: bpic17_logs_with_interventions_20_percent_test.csv\n",
      "load_logs: bpic17_logs_with_interventions_20_percent_train.csv\n",
      "load_model: bpic17_logs_with_interventions_40_percent.joblib\n",
      "load_logs: bpic17_logs_with_interventions_40_percent_test.csv\n",
      "load_logs: bpic17_logs_with_interventions_40_percent_train.csv\n",
      "load_model: bpic17_logs_with_interventions_66_percent.joblib\n",
      "load_logs: bpic17_logs_with_interventions_66_percent_test.csv\n",
      "load_logs: bpic17_logs_with_interventions_66_percent_train.csv\n",
      "\n",
      "testing with a sample\n",
      "recommendation: skip\n",
      "actual class: skip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "models_folder = './models/'\n",
    "logs_folder = './logs/'\n",
    "\n",
    "def load_model(file_path):\n",
    "    from joblib import load\n",
    "    print(f'load_model: {file_path}')\n",
    "    model = load(models_folder + file_path)\n",
    "    return model\n",
    "\n",
    "def load_logs(file_path):\n",
    "    print(f'load_logs: {file_path}')\n",
    "    data_df = pd.read_csv(logs_folder + file_path)\n",
    "    return data_df\n",
    "\n",
    "def classify_trace(running_trace, classifier_model):\n",
    "    trace_class = classifier_model.predict([running_trace])[0]\n",
    "    return trace_class\n",
    "\n",
    "def recommend_adaptation_3(running_trace, classifier_model):\n",
    "\n",
    "  recommended_adaptation_action = None\n",
    "\n",
    "  trace_class = classify_trace(running_trace, classifier_model)\n",
    "\n",
    "  recommended_adaptation_action = trace_class\n",
    "  \n",
    "  return recommended_adaptation_action\n",
    "\n",
    "models_file_names = [\n",
    "    'synthetic_logs_with_adaptations_20_percent.joblib',\n",
    "    'synthetic_logs_with_adaptations_40_percent.joblib',\n",
    "    'synthetic_logs_with_adaptations_66_percent.joblib',\n",
    "    'bpic17_logs_with_interventions_20_percent.joblib',\n",
    "    'bpic17_logs_with_interventions_40_percent.joblib',\n",
    "    'bpic17_logs_with_interventions_66_percent.joblib',\n",
    "]\n",
    "\n",
    "train_logs_file_names = [\n",
    "    'synthetic_logs_with_adaptations_20_percent_train.csv',\n",
    "    'synthetic_logs_with_adaptations_40_percent_train.csv',\n",
    "    'synthetic_logs_with_adaptations_66_percent_train.csv',\n",
    "    'bpic17_logs_with_interventions_20_percent_train.csv',\n",
    "    'bpic17_logs_with_interventions_40_percent_train.csv',\n",
    "    'bpic17_logs_with_interventions_66_percent_train.csv',\n",
    "]\n",
    "\n",
    "test_logs_file_names = [\n",
    "    'synthetic_logs_with_adaptations_20_percent_test.csv',\n",
    "    'synthetic_logs_with_adaptations_40_percent_test.csv',\n",
    "    'synthetic_logs_with_adaptations_66_percent_test.csv',\n",
    "    'bpic17_logs_with_interventions_20_percent_test.csv',\n",
    "    'bpic17_logs_with_interventions_40_percent_test.csv',\n",
    "    'bpic17_logs_with_interventions_66_percent_test.csv',\n",
    "]\n",
    "\n",
    "output_file_names = [\n",
    "    'synthetic_logs_with_adaptations_20_percent_output.csv',\n",
    "    'synthetic_logs_with_adaptations_40_percent_output.csv',\n",
    "    'synthetic_logs_with_adaptations_66_percent_output.csv',\n",
    "    'bpic17_logs_with_interventions_20_percent_output.csv',\n",
    "    'bpic17_logs_with_interventions_40_percent_output.csv',\n",
    "    'bpic17_logs_with_interventions_66_percent_output.csv',\n",
    "]\n",
    "\n",
    "models = []\n",
    "train_logs = []\n",
    "test_logs = []\n",
    "\n",
    "for index, model_file_name in enumerate(models_file_names):\n",
    "    model = load_model(model_file_name)\n",
    "    test_log = load_logs(test_logs_file_names[index])\n",
    "    train_log = load_logs(train_logs_file_names[index])\n",
    "\n",
    "    models.append(model)\n",
    "    test_logs.append(test_log)\n",
    "    train_logs.append(train_log)\n",
    "\n",
    "# test with a model and a trace\n",
    "print('\\ntesting with a sample')\n",
    "model = models[0]\n",
    "trace = test_logs[0].iloc[2]\n",
    "print('recommendation:', recommend_adaptation_3(trace[:-1], model))\n",
    "print('actual class:', trace[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do recommendations with models and store in output log files similar to test logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs: synthetic_logs_with_adaptations_20_percent_test.csv\n",
      "hit ratio: 0.53\n",
      "logs: synthetic_logs_with_adaptations_40_percent_test.csv\n",
      "hit ratio: 0.54\n",
      "logs: synthetic_logs_with_adaptations_66_percent_test.csv\n",
      "hit ratio: 0.5242424242424243\n",
      "logs: bpic17_logs_with_interventions_20_percent_test.csv\n",
      "hit ratio: 0.664756446991404\n",
      "logs: bpic17_logs_with_interventions_40_percent_test.csv\n",
      "hit ratio: 0.6859280483922318\n",
      "logs: bpic17_logs_with_interventions_66_percent_test.csv\n",
      "hit ratio: 0.5997491800115763\n"
     ]
    }
   ],
   "source": [
    "output_logs_folder = './output/'\n",
    "\n",
    "def print_decision_path(clf, X_test):\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "    node_indicator = clf.decision_path(X_test)\n",
    "    leaf_id = clf.apply(X_test)\n",
    "\n",
    "    sample_id = 0\n",
    "    # obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "    node_index = node_indicator.indices[\n",
    "        node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
    "    ]\n",
    "\n",
    "    print(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\n",
    "    for node_id in node_index:\n",
    "        # continue to the next node if it is a leaf node\n",
    "        if leaf_id[sample_id] == node_id:\n",
    "            continue\n",
    "\n",
    "        # check if value of the split feature for sample 0 is below threshold\n",
    "        if X_test[sample_id, feature[node_id]] <= threshold[node_id]:\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "\n",
    "        print(\n",
    "            \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n",
    "            \"{inequality} {threshold})\".format(\n",
    "                node=node_id,\n",
    "                sample=sample_id,\n",
    "                feature=feature[node_id],\n",
    "                value=X_test[sample_id, feature[node_id]],\n",
    "                inequality=threshold_sign,\n",
    "                threshold=threshold[node_id],\n",
    "            )\n",
    "        )\n",
    "\n",
    "def store_logs(dataframe, path):\n",
    "    dataframe.to_csv(output_logs_folder + path, columns=dataframe.columns, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize imputers\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer_knn = KNNImputer(n_neighbors=2)\n",
    "\n",
    "\n",
    "for index, test_logs in enumerate(test_logs):\n",
    "    log_name = test_logs_file_names[index]\n",
    "    print(f'logs: {log_name}')\n",
    "\n",
    "    ml_model = models[index]\n",
    "    train_logs_partition = train_logs[index].iloc[:,:-1] #train logs without class label\n",
    "\n",
    "    results = {\"adaptation_action_recommendation\": [],\n",
    "                \"hit\": []\n",
    "    }\n",
    "    hit_count = 0\n",
    "    logs_length = len(test_logs)\n",
    "\n",
    "    for row_index in range(logs_length):\n",
    "\n",
    "        process_case = test_logs.iloc[row_index, :-1] #case without class label\n",
    "\n",
    "        if index < 3: #synthetic_logs\n",
    "            na_column_names = ['trace:cycle_time']\n",
    "        else: #bpic17_logs_with_interventions\n",
    "            na_column_names = ['duration']\n",
    "\n",
    "        process_case_with_na = process_case.copy()\n",
    "        for column_name in na_column_names:\n",
    "            process_case_with_na.iloc[test_logs.columns.get_loc(column_name)] = np.nan\n",
    "\n",
    "        process_case_with_na = process_case_with_na.to_frame().T #change format from Series to Dataframe\n",
    "        # if index > 2:\n",
    "        #     print(f'before interpolation: {process_case_with_na.values}')\n",
    "\n",
    "        train_logs_partition_with_process_case = pd.concat([train_logs_partition, process_case_with_na])\n",
    "        process_case_interpolated = imputer_knn.fit_transform(train_logs_partition_with_process_case)[-1]\n",
    "        \n",
    "        # if index > 2:\n",
    "        #     print(f'after interpolation: {process_case_interpolated}')\n",
    "\n",
    "        recommendation = recommend_adaptation_3(process_case_interpolated, ml_model)\n",
    "\n",
    "        # print_decision_path(ml_model, [row[:-1]])\n",
    "\n",
    "        adaptation = test_logs.iloc[[row_index], -1:].values[0]\n",
    "        results[\"adaptation_action_recommendation\"].append(recommendation)\n",
    "\n",
    "        hit = 1 if recommendation == adaptation else 0\n",
    "        hit_count += hit\n",
    "        results[\"hit\"].append(hit)\n",
    "    \n",
    "    output_logs = test_logs.join(pd.DataFrame(results))\n",
    "    store_logs(output_logs, output_file_names[index])\n",
    "    print('hit ratio:', hit_count / logs_length)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrec-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d6367584639be9027060daf9528f44cac7e8714432a5c8c14804c351d2f41f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
